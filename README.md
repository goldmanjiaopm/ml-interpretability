# ml-interpretability

A project exploring model interpretability techniques for factual classification. The goal is to understand how models determine the factual accuracy of text statements by:

1. Using n-gram models to extract features from text
2. Building and hyperparameter tuning ensemble models
3. Exploring feature importance and model explainability through a variety of interpretability techniques

Inspired by Christoph Molnar's [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) book.